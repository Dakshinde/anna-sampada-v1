{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f97cd353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated data shape: (5000, 8)\n",
      "Spoilage distribution:\n",
      "Spoiled_flag\n",
      "Not Spoiled    2624\n",
      "Spoiled        2376\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# --- 1. Data Synthesis based on Dal Spoilage Logic ---\n",
    "def create_synthetic_dal_data(n_rows=5000):\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Define categories\n",
    "    storage_places = ['Room Temperature', 'Refrigerator', 'Freezer']\n",
    "    acidity_sources = ['Low/Normal', 'Moderate', 'High']\n",
    "    consistencies = ['Normal', 'Slightly Thickened', 'Watery', 'Slimy']\n",
    "    container_types = ['Steel/Metal', 'Plastic', 'Ceramic/Glass']\n",
    "    smells = ['Normal', 'Slightly Sour', 'Very Sour', 'Musty', 'Foul']\n",
    "\n",
    "    data = {\n",
    "        'Time_since_preparation_hours': np.random.uniform(0, 120, n_rows),\n",
    "        'Storage_place': np.random.choice(storage_places, n_rows, p=[0.4, 0.55, 0.05]),\n",
    "        'Acidity_source': np.random.choice(acidity_sources, n_rows, p=[0.6, 0.3, 0.1]),\n",
    "        'Consistency': np.random.choice(consistencies, n_rows, p=[0.7, 0.2, 0.08, 0.02]),\n",
    "        'Container_type': np.random.choice(container_types, n_rows, p=[0.5, 0.3, 0.2]),\n",
    "        'Smell': np.random.choice(smells, n_rows, p=[0.65, 0.25, 0.05, 0.03, 0.02]),\n",
    "        'Oil_separation': np.random.uniform(0.0, 1.0, n_rows)\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Apply real-world spoilage rules to generate 'Spoiled_flag' (Target)\n",
    "    def determine_spoilage(row):\n",
    "        # Rule 1: Long time at Room Temp (>24hrs)\n",
    "        if row['Storage_place'] == 'Room Temperature' and row['Time_since_preparation_hours'] > 24:\n",
    "            return 'Spoiled'\n",
    "        \n",
    "        # Rule 2: Moderate time at Room Temp + High Acidity/Bad Smell (>8hrs)\n",
    "        if row['Storage_place'] == 'Room Temperature' and row['Time_since_preparation_hours'] > 8 and \\\n",
    "           (row['Acidity_source'] == 'High' or row['Smell'] in ['Very Sour', 'Foul']):\n",
    "            return 'Spoiled'\n",
    "        \n",
    "        # Rule 3: Extreme indicators (Slimy or Foul Smell)\n",
    "        if row['Consistency'] == 'Slimy' or row['Smell'] == 'Foul':\n",
    "            return 'Spoiled'\n",
    "\n",
    "        # Rule 4: Very long time in Refrigerator + High Acidity (>72hrs)\n",
    "        if row['Storage_place'] == 'Refrigerator' and row['Time_since_preparation_hours'] > 72 and \\\n",
    "           row['Acidity_source'] == 'High':\n",
    "            return 'Spoiled'\n",
    "\n",
    "        # Baseline: Use a probabilistic model based on time and oil separation\n",
    "        time_factor = row['Time_since_preparation_hours'] / 120.0\n",
    "        oil_factor = row['Oil_separation']\n",
    "        \n",
    "        # Higher probability of spoilage if time is long and oil separation is high\n",
    "        spoil_prob = time_factor * 0.4 + oil_factor * 0.3\n",
    "        \n",
    "        # Adjust probability based on storage\n",
    "        if row['Storage_place'] == 'Refrigerator':\n",
    "            spoil_prob *= 0.5\n",
    "        elif row['Storage_place'] == 'Freezer':\n",
    "            spoil_prob *= 0.1\n",
    "        \n",
    "        if np.random.rand() < spoil_prob:\n",
    "            return 'Spoiled'\n",
    "        \n",
    "        return 'Not Spoiled'\n",
    "\n",
    "    df['Spoiled_flag'] = df.apply(determine_spoilage, axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Create the data or load your existing data (uncomment the line below if you use your 5000 rows)\n",
    "# df = pd.read_csv('dal_spoilage_data.csv')\n",
    "df = create_synthetic_dal_data(n_rows=5000)\n",
    "\n",
    "print(f\"Generated data shape: {df.shape}\")\n",
    "print(\"Spoilage distribution:\")\n",
    "print(df['Spoiled_flag'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "11d9e310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing successful.\n"
     ]
    }
   ],
   "source": [
    "# --- 2. CORRECTED Preprocessing and Splitting ---\n",
    "\n",
    "# Define columns by type\n",
    "numerical_cols = ['Time_since_preparation_hours', 'Oil_separation']\n",
    "categorical_cols = ['Storage_place', 'Acidity_source', 'Consistency', 'Container_type', 'Smell']\n",
    "\n",
    "# Ensure categorical columns are explicitly of type 'category'.\n",
    "df[categorical_cols] = df[categorical_cols].astype('category')\n",
    "\n",
    "# Create the Column Transformer\n",
    "# Removed 'sparse_output=False' for older scikit-learn version compatibility.\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        # Use StandardScaler for numerical features\n",
    "        ('num', StandardScaler(), numerical_cols),\n",
    "        # Use OneHotEncoder for categorical features. \n",
    "        # handle_unknown='ignore' prevents errors during transformation if a new category appears.\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "# Encode the target variable\n",
    "le = LabelEncoder()\n",
    "df['Spoiled_flag_Encoded'] = le.fit_transform(df['Spoiled_flag'])\n",
    "\n",
    "X = df.drop(['Spoiled_flag', 'Spoiled_flag_Encoded'], axis=1)\n",
    "y = df['Spoiled_flag_Encoded']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Fit the preprocessor to the training data and transform all data\n",
    "try:\n",
    "    X_train_processed = preprocessor.fit_transform(X_train)\n",
    "    X_test_processed = preprocessor.transform(X_test)\n",
    "    print(\"Preprocessing successful.\")\n",
    "    \n",
    "    # Get feature names after one-hot encoding for the final model (used for saving)\n",
    "    # Note: 'get_feature_names_out' might also require a newer scikit-learn version.\n",
    "    # We will try to get feature names for robust saving.\n",
    "    try:\n",
    "        feature_names = preprocessor.get_feature_names_out()\n",
    "    except AttributeError:\n",
    "        # Fallback if get_feature_names_out is not available in your version\n",
    "        print(\"Warning: Could not use preprocessor.get_feature_names_out(). Feature name saving may be affected.\")\n",
    "        feature_names = None\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"ERROR during Preprocessing: {e}\")\n",
    "    print(\"Please check data types and ensure all required libraries are imported (StandardScaler, OneHotEncoder).\")\n",
    "    raise # Re-raise the error for debugging\n",
    "\n",
    "# IMPORTANT: Ensure you have imported OneHotEncoder at the top of your script/notebook:\n",
    "# from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c7aa61c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting GridSearchCV for XGBoost...\n",
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "\n",
      "--- Tuned XGBoost Results ---\n",
      "Best Hyperparameters found: {'learning_rate': 0.05, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 200}\n",
      "Tuned XGBoost Test Accuracy: **0.9050**\n",
      "\n",
      "Classification Report (0=Not Spoiled, 1=Spoiled):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " Not Spoiled       0.85      1.00      0.92       525\n",
      "     Spoiled       0.99      0.80      0.89       475\n",
      "\n",
      "    accuracy                           0.91      1000\n",
      "   macro avg       0.92      0.90      0.90      1000\n",
      "weighted avg       0.92      0.91      0.90      1000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shivam\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [19:16:16] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Tuned XGBoost Training with Grid Search ---\n",
    "\n",
    "# Define the model parameters to search\n",
    "param_grid = {\n",
    "    'n_estimators': [150, 200, 300],\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'min_child_weight': [1, 3],\n",
    "}\n",
    "\n",
    "# Initialize XGBoost model\n",
    "xgb_clf = xgb.XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss',\n",
    "    random_state=42,\n",
    "    # scale_pos_weight is helpful for class imbalance, but we assume it's relatively balanced for now\n",
    ")\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb_clf,\n",
    "    param_grid=param_grid,\n",
    "    scoring='accuracy',\n",
    "    cv=5, # 5-fold Cross-Validation\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"Starting GridSearchCV for XGBoost...\")\n",
    "grid_search.fit(X_train_processed, y_train)\n",
    "\n",
    "# Get the best model\n",
    "best_xgb_model = grid_search.best_estimator_\n",
    "\n",
    "# Final Evaluation\n",
    "y_pred_tuned_xgb = best_xgb_model.predict(X_test_processed)\n",
    "accuracy_tuned_xgb = accuracy_score(y_test, y_pred_tuned_xgb)\n",
    "\n",
    "print(\"\\n--- Tuned XGBoost Results ---\")\n",
    "print(f\"Best Hyperparameters found: {grid_search.best_params_}\")\n",
    "print(f\"Tuned XGBoost Test Accuracy: **{accuracy_tuned_xgb:.4f}**\")\n",
    "print(\"\\nClassification Report (0=Not Spoiled, 1=Spoiled):\")\n",
    "print(classification_report(y_test, y_pred_tuned_xgb, target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ad760263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model training and saving complete. Files: 'dal_spoilage_final_model.joblib', 'dal_spoilage_preprocessor.joblib', etc. are ready.\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Saving All Components for Deployment ---\n",
    "\n",
    "# Save the Tuned XGBoost model\n",
    "joblib.dump(best_xgb_model, 'dal_spoilage_final_model.joblib')\n",
    "\n",
    "# Save the entire preprocessor object (contains scaling and one-hot encoding logic)\n",
    "joblib.dump(preprocessor, 'dal_spoilage_preprocessor.joblib')\n",
    "\n",
    "# Save the LabelEncoder\n",
    "joblib.dump(le, 'dal_spoilage_label_encoder.joblib')\n",
    "\n",
    "# Save the feature names (needed if you need to inspect features, but less critical than the preprocessor)\n",
    "joblib.dump(feature_names, 'dal_spoilage_feature_names.joblib')\n",
    "\n",
    "print(\"\\nModel training and saving complete. Files: 'dal_spoilage_final_model.joblib', 'dal_spoilage_preprocessor.joblib', etc. are ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d4cdb1",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'ML/dal/dal_spoilage_preprocessor.joblib'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Load the preprocessor you already trained and saved\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m preprocessor = \u001b[43mjoblib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mML/dal/dal_spoilage_preprocessor.joblib\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Extract feature names\u001b[39;00m\n\u001b[32m      8\u001b[39m columns = \u001b[38;5;28mlist\u001b[39m(preprocessor.get_feature_names_out())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\numpy_pickle.py:735\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(filename, mmap_mode, ensure_native_byte_order)\u001b[39m\n\u001b[32m    733\u001b[39m         obj = _unpickle(fobj, ensure_native_byte_order=ensure_native_byte_order)\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m735\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    736\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m _validate_fileobject_and_memmap(f, filename, mmap_mode) \u001b[38;5;28;01mas\u001b[39;00m (\n\u001b[32m    737\u001b[39m             fobj,\n\u001b[32m    738\u001b[39m             validated_mmap_mode,\n\u001b[32m    739\u001b[39m         ):\n\u001b[32m    740\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fobj, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    741\u001b[39m                 \u001b[38;5;66;03m# if the returned file object is a string, this means we\u001b[39;00m\n\u001b[32m    742\u001b[39m                 \u001b[38;5;66;03m# try to load a pickle file generated with an version of\u001b[39;00m\n\u001b[32m    743\u001b[39m                 \u001b[38;5;66;03m# Joblib so we load it with joblib compatibility function.\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'ML/dal/dal_spoilage_preprocessor.joblib'"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import json\n",
    "\n",
    "# Load the preprocessor you already trained and saved\n",
    "# preprocessor = joblib.load(\"ML/dal/dal_spoilage_preprocessor.joblib\")\n",
    "\n",
    "# Extract feature names\n",
    "columns = list(preprocessor.get_feature_names_out())\n",
    "\n",
    "# Save them to JSON\n",
    "with open(\"ML/dal/dal_model_columns.json\", \"w\") as f:\n",
    "    json.dump(columns, f, indent=4)\n",
    "\n",
    "print(\"âœ… dal_model_columns.json created successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
